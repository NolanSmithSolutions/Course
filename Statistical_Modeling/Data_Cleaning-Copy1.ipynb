{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![StatModels](https://www.durhamtech.edu/themes/custom/durhamtech/images/durham-tech-logo-web.svg) \n",
    "\n",
    "## Applications - Statistical Modeling\n",
    "\n",
    "This lecture provides foundational knowledge and examples of machine learning modeling concepts by examining stock price data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### Jupyter Overview\n",
    "#### <a href='#1'>Useful Links</a>\n",
    "#### <a href='#2'>Introduction to Jupyter Notebooks</a>\n",
    "#### <a href='#3'>Cell Types</a>\n",
    "* Markdown \n",
    "* Code\n",
    "    1. Running One Cell\n",
    "    2. Other Run Options\n",
    "\n",
    "#### <a href='#4'>Tips and Tricks</a>\n",
    "\n",
    "#### <a href='#55'>Weekly Readings/Videos</a>\n",
    "#### <a href='#56'>Extra Practice</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "![FunnyML](https://www.meme-arsenal.com/memes/11f11b5d16eef661677e4c9e989dd2b3.jpg) \n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. Weather: https://www.weather.gov/wrh/climate?wfo=okx\n",
    "2. SP 500 Components: https://datahub.io/core/s-and-p-500-companies\n",
    "3. SP 500 Company Info: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
    "4. FRED https://fred.stlouisfed.org/docs/api/fred/series.html\n",
    "5. TD Ameritrade Data Dicitionary https://developer.tdameritrade.com/content/streaming-data#_Toc504640567\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/yahoo-finance-api2/\n",
    "# https://github.com/pkout/yahoo_finance_api2\n",
    "\n",
    "# Uncomment below if you don't have yahoo finance api installed\n",
    "# pip install yahoo_finance_api2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import time\n",
    "import math\n",
    "\n",
    "from yahoo_finance_api2 import share\n",
    "from yahoo_finance_api2.exceptions import YahooFinanceError\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "key = 'RGOLSJPSTGVAN4NTN4DLWJE71SU7SIH0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view file contents\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=pd.read_csv(\"constituents_csv.csv\")\n",
    "print(len(tickers))\n",
    "tickers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_info=pd.read_csv(\"sp500_info.csv\")\n",
    "print(len(ticker_info))\n",
    "ticker_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=pd.merge(tickers,ticker_info,on='Symbol',how='inner')\n",
    "print(len(tickers))\n",
    "tickers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ticker_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_price_hist(ticker,period,key,row_count='Blank'):\n",
    "    time.sleep(1)\n",
    "    endpoint = 'https://api.tdameritrade.com/v1/marketdata/'+ticker+'/pricehistory'\n",
    "\n",
    "    ##Define Payload\n",
    "    payload = {'apikey': key,\n",
    "    'periodType': 'year',\n",
    "    'period':period,\n",
    "    'frequencyType':'daily'}\n",
    "\n",
    "    ### make request\n",
    "    try:\n",
    "        content = requests.get(url = endpoint, params = payload)\n",
    "    except:\n",
    "        print('API error, please review.')\n",
    "        \n",
    "    ### Convert to dictionary\n",
    "    dictlist = []\n",
    "    data = content.json()\n",
    "\n",
    "    for key, value in data.items():\n",
    "        temp = [key,value]\n",
    "        dictlist.append(temp)\n",
    "        \n",
    "    try:\n",
    "        hist_data = pd.DataFrame(dictlist[0][1])\n",
    "        hist_data['datetime'] = pd.to_datetime(hist_data['datetime'],unit='ms')\n",
    "        hist_data.sort_values(by=['datetime'],ascending=False)\n",
    "        hist_data=hist_data.sort_values(by=['datetime'],ascending=True).reset_index()\n",
    "        hist_data['Date']=hist_data['datetime'].dt.date\n",
    "        hist_data=hist_data.drop(['index','datetime'],axis=1)\n",
    "        hist_data['ticker'] = ticker\n",
    "        if row_count!='Blank':\n",
    "            return hist_data.tail(row_count)\n",
    "        else:\n",
    "            return hist_data\n",
    "    except:\n",
    "        df = pd.DataFrame()\n",
    "        print('running except clause')\n",
    "        return df\n",
    "    \n",
    "def get_fundamental_from_td(ticker,key):\n",
    "    time.sleep(1)\n",
    "    endpoint = 'https://api.tdameritrade.com/v1/instruments'\n",
    "    projection = 'fundamental'\n",
    "\n",
    "    ##Define Payload\n",
    "    payload = {'apikey': key,\n",
    "               'symbol' : ticker,\n",
    "                'projection': projection,\n",
    "                }\n",
    "    \n",
    "    ### make request\n",
    "    try:\n",
    "        content = requests.get(url = endpoint, params = payload)\n",
    "    except:\n",
    "        print('API error, please review.')\n",
    "        \n",
    "    ### Convert to dictionary\n",
    "    dictlist = []\n",
    "    data = content.json()\n",
    "    for key, value in data.items():\n",
    "        temp = [key,value]\n",
    "        dictlist.append(temp)\n",
    "        \n",
    "    try:\n",
    "        df = pd.DataFrame(dictlist[0][1]).T.reset_index(drop=True).iloc[0]\n",
    "        return df\n",
    "    except:\n",
    "        print(dictlist)\n",
    "        df = pd.DataFrame()\n",
    "        print(ticker + \" not valid.\")\n",
    "        return df\n",
    "    \n",
    "def get_yahoo_history(share_name):\n",
    "    print(\"Pulling history\")\n",
    "\n",
    "    if share_name[-1]=='2':\n",
    "        my_share = share.Share(share_name[:-1])\n",
    "    else:\n",
    "        my_share = share.Share(share_name)\n",
    "    symbol_data = None\n",
    "\n",
    "    try:\n",
    "        symbol_data = my_share.get_historical(share.PERIOD_TYPE_YEAR,\n",
    "                                              30000,\n",
    "                                              share.FREQUENCY_TYPE_DAY,\n",
    "                                              1)\n",
    "        df = pd.DataFrame(symbol_data)\n",
    "        df['timestamp'] = df['timestamp'].astype(str)\n",
    "        df['timestamp'] = df['timestamp'].map(lambda x: x[:-3])\n",
    "        df['Date'] =df['timestamp'].astype('int')\n",
    "        df['Date'] = pd.to_datetime(df['Date'],unit='s')\n",
    "        df=df.sort_values(by=['Date'],ascending=True)\n",
    "        df['Date']=df['Date'].dt.date\n",
    "        df=df.drop(['timestamp'],axis=1)\n",
    "        df['ticker']=str(share_name.upper())\n",
    "    except YahooFinanceError as e:\n",
    "        print(e.message)\n",
    "        sys.exit(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ticker=tickers['Symbol'][1]\n",
    "ticker='^GSPC'\n",
    "ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers.iloc[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_fundamental_from_td('AOS',key)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ticker)\n",
    "#td_data=get_td_price_hist(ticker,1,key,43)\n",
    "td_data=get_td_price_hist(ticker,1,key)\n",
    "print(len(td_data))\n",
    "td_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_data=get_yahoo_history(ticker)\n",
    "print(len(yahoo_data))\n",
    "yahoo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.merge(yahoo_data,td_data,on=['Date'],how='outer',indicator=True)\n",
    "#pd.merge(yahoo_data.tail(10),td_data.tail(10),on=['Date'],how='outer',indicator=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.read_excel('nyc_temp.xlsx')\n",
    "prec=pd.read_excel('nyc_precip.xlsx')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=prec.replace('', np.nan).set_index('Year').stack().reset_index(name='prec').rename(columns={\"level_1\": \"Month\"})\n",
    "temp=temp.replace('', np.nan).set_index('Year').stack().reset_index(name='temp').rename(columns={\"level_1\": \"Month\"})\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42684530/convert-a-column-in-a-python-pandas-from-string-month-into-int\n",
    "from calendar import month_abbr\n",
    "\n",
    "lower_ma = [m.lower() for m in month_abbr]\n",
    "\n",
    "prec['Month']=prec['Month'].str.lower().map(lambda m: lower_ma.index(m)).astype('Int8')\n",
    "temp['Month']=temp['Month'].str.lower().map(lambda m: lower_ma.index(m)).astype('Int8')\n",
    "\n",
    "\n",
    "yahoo_data['month']=pd.to_datetime(yahoo_data.Date).dt.month\n",
    "yahoo_data['year']=pd.to_datetime(yahoo_data.Date).dt.year\n",
    "yahoo_data['quarter']=pd.to_datetime(yahoo_data.Date).dt.quarter\n",
    "yahoo_data=pd.merge(yahoo_data,prec,left_on=['month','year'],right_on=['Month','Year'],how='left')\n",
    "yahoo_data=yahoo_data.drop(['Month','Year'],axis=1)\n",
    "yahoo_data=pd.merge(yahoo_data,temp,left_on=['month','year'],right_on=['Month','Year'],how='left')\n",
    "yahoo_data=yahoo_data.drop(['Month','Year'],axis=1)\n",
    "yahoo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_data[(yahoo_data['prec']=='M')|(yahoo_data['temp']=='M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.read_csv('DFF.csv')\n",
    "\n",
    "unrate=pd.read_csv('UNRATE.csv')\n",
    "unrate['month']=pd.to_datetime(unrate.DATE).dt.month\n",
    "unrate['year']=pd.to_datetime(unrate.DATE).dt.year\n",
    "unrate.drop('DATE',axis=1,inplace=True)\n",
    "\n",
    "gdp=pd.read_csv('GDPC1.csv')\n",
    "gdp['quarter']=pd.to_datetime(gdp.DATE).dt.quarter\n",
    "gdp['year']=pd.to_datetime(gdp.DATE).dt.year\n",
    "gdp.drop('DATE',axis=1,inplace=True)\n",
    "\n",
    "yahoo_data=pd.merge(yahoo_data,dff,left_on=pd.to_datetime(yahoo_data.Date),right_on=pd.to_datetime(dff.DATE),how='inner')\n",
    "yahoo_data=pd.merge(yahoo_data,unrate,on=['month','year'],how='inner')\n",
    "#yahoo_data=pd.merge(yahoo_data,gdp,on=['quarter','year'],how='inner')\n",
    "yahoo_data.drop(['key_0','DATE','month','quarter','year'],axis=1,inplace=True)\n",
    "yahoo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html\n",
    "\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html\n",
    "# https://stackoverflow.com/questions/61319814/moving-average-in-pandas-issue-with-first-and-last-rows\n",
    "\n",
    "# 1 year\n",
    "days_out=252\n",
    "\n",
    "yahoo_data['volume_moving'] = yahoo_data['volume'].rolling(days_out).mean().shift(periods=1)\n",
    "yahoo_data['volume_moving_std']=yahoo_data['volume'].rolling(days_out).std().shift(periods=1)\n",
    "yahoo_data['close_moving'] = yahoo_data['close'].rolling(days_out).mean().shift(periods=1)\n",
    "yahoo_data['close_moving_std']=yahoo_data['close'].rolling(days_out).std().shift(periods=1)\n",
    "\n",
    "# https://stackoverflow.com/questions/42138357/pandas-rolling-slope-calculation\n",
    "\n",
    "def calc_slope(x):\n",
    "    slope = np.polyfit(range(len(x)), x, 1)[0]\n",
    "    return slope\n",
    "\n",
    "yahoo_data['volume_slope'] = yahoo_data['volume'].rolling(days_out).apply(calc_slope).shift(periods=1)\n",
    "yahoo_data['close_slope'] = yahoo_data['close'].rolling(days_out).apply(calc_slope).shift(periods=1)\n",
    "\n",
    "yahoo_data['close_future'] = yahoo_data['close'].shift(periods=-days_out)\n",
    "\n",
    "yahoo_data.drop(columns=['high','low','volume','ticker','Date'],inplace=True)\n",
    "yahoo_data.dropna(inplace=True)\n",
    "yahoo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "![FunnyReg](https://memegenerator.net/img/instances/49880835.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def data_split(df,y_var,scale=False):\n",
    "    reg_df=df.copy()\n",
    "    \n",
    "    # train test split\n",
    "    #y=reg_df.pop(y_var)\n",
    "    #X=reg_df\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)\n",
    "    \n",
    "    # first 80% train\n",
    "    x_train = reg_df.head(int(len(reg_df)*(0.8)))\n",
    "    x_test = reg_df.iloc[max(x_train.index):]\n",
    "    y_train = x_train.pop(y_var)\n",
    "    y_test = x_test.pop(y_var)\n",
    "    \n",
    "    colz=x_train.columns\n",
    "    \n",
    "    if scale:\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test, colz\n",
    "\n",
    "def regression(x_train, x_test, y_train, y_test, colz):\n",
    "\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(x_test)\n",
    "\n",
    "    print(\"Number of training records:\", len(y_train))\n",
    "    print(\"Number of testing records:\",len(y_test))\n",
    "    print(\"\\nLinear Regression Results\")\n",
    "\n",
    "    # The coefficients\n",
    "    print('\\nCoefficients:')\n",
    "    for x,y in zip(colz,regr.coef_):\n",
    "        print(x,y)\n",
    "    \n",
    "    #The intercept\n",
    "    print('\\nIntercept:', regr.intercept_)      \n",
    "    print('\\nLinear Regression R^2 score on training data: %.4f' % regr.score(x_train,y_train))\n",
    "    print('Linear Regression R^2 score on test data: %.4f' % r2_score(y_test, y_pred))\n",
    "    \n",
    "def random_forest(x_train, x_test, y_train, y_test, colz, cat=False, est=10):\n",
    "    # If continous y variable\n",
    "    random_forest = RandomForestRegressor(n_estimators=est)\n",
    "    \n",
    "    # If categorical y variable\n",
    "    if cat:\n",
    "        random_forest = RandomForestClassifier(n_estimators=est)\n",
    "    \n",
    "    random_forest.fit(x_train, y_train)\n",
    "    train_acc = random_forest.score(x_train, y_train)\n",
    "    test_acc = random_forest.score(x_test, y_test)\n",
    "    \n",
    "    y_pred = random_forest.predict(x_test)\n",
    "    \n",
    "    print('Random Forest Results:')\n",
    "    \n",
    "    print('Training acuracy= ',train_acc)\n",
    "    print('Test accuracy= ',test_acc)\n",
    "\n",
    "    features = x_train.columns\n",
    "    importances = random_forest.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    plt.subplots(figsize=(15, 11))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "def cart(x_train, x_test, y_train, y_test, colz, cat=False):\n",
    "    # if continuous variable\n",
    "    cart = DecisionTreeRegressor(random_state=12)\n",
    "    if cat:\n",
    "        cart = DecisionTreeClassifier(random_state=12)\n",
    "    cart.fit(x_train, y_train)\n",
    "    train_acc = cart.score(x_train, y_train)\n",
    "    test_acc = cart.score(x_test, y_test)\n",
    "    \n",
    "    y_pred = cart.predict(x_test)\n",
    "    \n",
    "    print('CART Results:')\n",
    "    \n",
    "    print('CART training acuracy= ',train_acc)\n",
    "    print('CART test accuracy= ',test_acc)\n",
    "    \n",
    "def lasso(x_train, x_test, y_train, y_test, colz):\n",
    "    lasso = linear_model.Lasso(alpha=0.25)\n",
    "    lasso.fit(x_train, y_train)\n",
    "    y_pred = lasso.predict(x_test)\n",
    "    train_acc = lasso.score(x_train, y_train)\n",
    "    test_acc = lasso.score(x_test, y_test)\n",
    "    \n",
    "    print('Lasso Regression Results:')\n",
    "    print('Training acuracy =',train_acc)\n",
    "    print('Test accuracy =',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, colz = data_split(yahoo_data,'close_future')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(x_train, x_test, y_train, y_test, colz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression(x_train, x_test, y_train, y_test, colz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso(x_train, x_test, y_train, y_test, colz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53\n",
    "def perceptron(x_train, x_test, y_train, y_test, colz):\n",
    "    perceptron = Perceptron(max_iter=13)\n",
    "    perceptron.fit(x_train, y_train)\n",
    "    perceptron_train_acc = perceptron.score(x_train, y_train)\n",
    "    perceptron_test_acc = perceptron.score(x_test, y_test)\n",
    "    print ('perceptron training acuracy= ',perceptron_train_acc)\n",
    "    print('perceptron test accuracy= ',perceptron_test_acc)\n",
    "    \n",
    "def logreg(x_train, x_test, y_train, y_test, colz):\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(x_train, y_train)\n",
    "    logreg_train_acc = logreg.score(x_train, y_train)\n",
    "    logreg_test_acc = logreg.score(x_test, y_test)\n",
    "    print ('logreg training acuracy= ',logreg_train_acc)\n",
    "    print('logreg test accuracy= ',logreg_test_acc)\n",
    "    \n",
    "def svm(x_train, x_test, y_train, y_test, colz):\n",
    "    print(\"SVM results:\")\n",
    "    \n",
    "    svc = SVC()                                                  \n",
    "    svc.fit(x_train, y_train)                                    \n",
    "    svc_train_acc = svc.score(x_train, y_train)\n",
    "    svc_test_acc = svc.score(x_test, y_test)\n",
    "    print ('SVM training acuracy= ',svc_train_acc)\n",
    "    print('SVM test accuracy= ',svc_test_acc)\n",
    "    \n",
    "def knn(x_train, x_test, y_train, y_test, colz,neighbors=3):\n",
    "    knn = KNeighborsClassifier(n_neighbors = neighbors)                  \n",
    "    knn.fit(x_train, y_train)                                    \n",
    "    knn_train_acc = knn.score(x_train, y_train)\n",
    "    knn_test_acc = knn.score(x_test, y_test)\n",
    "    print ('KNN training acuracy= ',knn_train_acc)\n",
    "    print('KNN test accuracy= ',knn_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.investopedia.com/ask/answers/042415/what-average-annual-return-sp-500.asp\n",
    "yahoo_data['best']=np.where(((yahoo_data.close_future-yahoo_data.close)/yahoo_data.close) > .08, 1, 0)\n",
    "#yahoo_data['close_moving']=pd.cut(yahoo_data['close_moving'],3,labels=[0,1,2])\n",
    "yahoo_data.drop(columns=['close','close_future'],inplace=True)\n",
    "yahoo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_data['best'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, colz = data_split(yahoo_data,'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron(x_train, x_test, y_train, y_test, colz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg(x_train, x_test, y_train, y_test, colz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart(x_train, x_test, y_train, y_test, colz, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(x_train, x_test, y_train, y_test, colz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(x_train, x_test, y_train, y_test, colz,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(x_train, x_test, y_train, y_test, colz, True, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize model constructor\n",
    "def neural_netter(x_train, x_test, y_train, y_test, colz):\n",
    "    inp_sh=np.array(x_train).shape[1]\n",
    "    print(\"Neural Network results:\")\n",
    "\n",
    "    model = Sequential()\n",
    "    # Add layers sequentially\n",
    "    model.add(Dense(500, activation='relu', \\\n",
    "                        input_shape=(inp_sh,)))\n",
    "    # Second\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    # Third\n",
    "    model.add(Dense(250, activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    # train the model\n",
    "    NO_EPOCHS = 20\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=30,\n",
    "                        epochs=NO_EPOCHS,\n",
    "                        validation_split=0.2)\n",
    "\n",
    "    yhat_probs = model.predict(x_test, verbose=0)\n",
    "    yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "    print(\"Test accuracy:\",model.evaluate(x_test,y_test,verbose=0)[1])\n",
    "    ConfusionMatrix=pd.DataFrame(confusion_matrix(y_test, yhat_classes),columns=['Predicted 0','Predicted 1'],index=['Actual 0','Actual 1'])\n",
    "    print ('Confusion matrix of test data is: \\n',ConfusionMatrix)\n",
    "    print(\"Average precision for the 2 classes is - \", precision_score(y_test, yhat_classes, average = None) )\n",
    "    print(\"Average recall for the 2 classes is - \", recall_score(y_test, yhat_classes, average = None) )\n",
    "    def plot_loss_acc(hist):\n",
    "        f, ax = plt.subplots()\n",
    "        ax.plot([None] + hist.history['acc'], 'o-')\n",
    "        ax.plot([None] + hist.history['val_acc'], 'x-')\n",
    "        # Plot legend and use the best location automatically: loc = 0.\n",
    "        ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
    "        ax.set_title('Training/Validation acc per Epoch')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Acc') \n",
    "        plt.plot()\n",
    "\n",
    "        f, ax = plt.subplots()\n",
    "        ax.plot([None] + hist.history['loss'], 'o-',c='r')\n",
    "        ax.plot([None] + hist.history['val_loss'], 'x-',c='g')\n",
    "        # Plot legend and use the best location automatically: loc = 0.\n",
    "        ax.legend(['Train loss', 'Validation loss'], loc = 0)\n",
    "        ax.set_title('Training/Validation loss per Epoch')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss') \n",
    "        plt.plot()\n",
    "    plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, colz = data_split(yahoo_data,'best',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_netter(x_train, x_test, y_train, y_test, colz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------PRACTICE-------------\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='55'></a>\n",
    "# Weekly Readings/Videos\n",
    "\n",
    "https://blog.trinket.io/why-python/\n",
    "    \n",
    "https://towardsdatascience.com/top-16-python-applications-in-real-world-a0404111ac23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='56'></a>\n",
    "# Extra Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
