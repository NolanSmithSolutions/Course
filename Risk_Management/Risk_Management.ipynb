{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Management & Applications\n",
    "As long as there have been economies, there has been financial risk.  Risk generally comes from a variety of sources including but not limited to:\n",
    "    1. Market risk - risk from price changes and losses on owned securities\n",
    "    2. Credit risk - risk of clients defaulting or being downgraded\n",
    "    3. Operational risk - risk associated with processing and systems failures\n",
    "    4. Legal risk - contract violation and any regulatory exposure through non-compliance\n",
    "Market risk can be broken down further into a handful of main groups:\n",
    "    1. Equity risk\n",
    "    2. Interest rate risk\n",
    "    3. Currency (Forex) risk\n",
    "    4. Commodity risk\n",
    "In this lecture we will attempt to model aggregate risk, and have a general discussion on what insights modeling can provide us.  Can you think of any other risks not outlined above?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value at Risk\n",
    "We briefly touched on this concept in our TD lecture, but here we will give it a more thorough analysis on the theory behind why it should work.  THere are two main ways to communicate VAR:\n",
    "    1. Maximum Loss - Here we state that the VAR is the max loss that can be incurred for a security or portfolio over some holding period given a specific confidence level.  For example, we will state that we are 95% confident that the loss, as measured by VAR, won't exceed $2MM over the next month.\n",
    "\n",
    "    2. Minimum Loss - Here we state that the VAR is the min loss that can be incurred for a security or portfolio over some period given a significance level.  For example, we will state that there is a 5% chance that the loss, as measured by VAR, will exceed $2MM over the next month.  \n",
    "    \n",
    "Measured needed:\n",
    "    1. Current Asset(s) price(s)\n",
    "    2. Expected Returns (sometimes assumed 0 for conservatism)\n",
    "    3. Volatility of returns (standard deviation)\n",
    "    4. Covariance of all assets\n",
    "    \n",
    "Key Assumptions:\n",
    "    1.  It is assumed, for this model, that standard conditions will continue into the projection period.  This is manifested by using historical measures to measure volatility going forward.  Generally, when black swan events occur, this model will not be an accurate predictor of near term loss expectations.\n",
    "    2. The underlying distribution is normal.\n",
    "    \n",
    "One important issue in computing VAR is a one tailed test.  Also, generally in practice, it's assumed that the standard deviation comes from a large sample, so the t factor (for large samples) approaches the standard normal distribution, such that, for 5% VAR it is 1.65, and for 1% VAR it is 2.33. \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for normality\n",
    "We will begin by testing for normality of an underlying data set.  In order to test this, we will implement the Kolmogorov-Smirnov test (also known as the KS test).  While this test can theoretically be used to test goodness of fit to any distribution, we will apply it to the normal distribution.\n",
    "\n",
    "The following is how to implement the test:\n",
    "    1. Specify the cumulitive density function (for us it will be normal)\n",
    "    2. To set up the theoretical normal distribution, we need to standardize the observations to a standard normal distribution and then look up the associated cumulative probability iin the tables\n",
    "    3. Then take the absolute difference between the actual and the theoretical cumulative probability distribution \n",
    "    4. Find the maximum difference (D) from the above step\n",
    "    5. Use this D value to compare to the critical KS table value.  If D is greater than the critical table value, then we reject the null hypothesis that the actual data is normally distributed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.06\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, we will assume a portfolio of $10,000, and only holding 1 stock: T\n",
    "\n",
    "## Read in Data\n",
    "spy_data = pd.read_csv('SPY_Data.csv')\n",
    "\n",
    "##Run KS test\n",
    "x = spy_data['Return']\n",
    "ks = sm.stats.diagnostic.kstest_normal(x, dist='norm', pvalmethod='table')[1]  ##test\n",
    "print(\"p-value: \" + str(round(ks,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our p-value for the test is greater than 0.05, we fail to reject our null hypothesis that the data is normally distributed.  While this doesn't mean that our data is definitely normal, we can feel more confident. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Stationary Data\n",
    "The next key assumption we are implicitely making is that the historical data is a meaningful predictor of the future.  To test this, we will use two tests:\n",
    "    1. Autocorrelation function test (Durban-Watson test)\n",
    "    2. Unit Root test\n",
    "Dealing with non-stationary data (i.e. the mean and variance of the underlying distribution is not static) is an especially common issue with time series modeling so being confident in the underlying data is critical.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "Our first test will be the autocorrelation  test.  In order to conduct this test, we will see if there is a pattern in the error terms of a time series regression of SPY return vs. SPY return of the prior day.  If there is a relationship in the error terms, then we can conclude our data is non-stationary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                 Return   R-squared (uncentered):                   0.004\n",
      "Model:                            OLS   Adj. R-squared (uncentered):             -0.000\n",
      "Method:                 Least Squares   F-statistic:                             0.9172\n",
      "Date:                Tue, 05 Apr 2022   Prob (F-statistic):                       0.339\n",
      "Time:                        16:56:02   Log-Likelihood:                          819.71\n",
      "No. Observations:                 252   AIC:                                     -1637.\n",
      "Df Residuals:                     251   BIC:                                     -1634.\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "1_Day_Lag_Return     0.0604      0.063      0.958      0.339      -0.064       0.185\n",
      "==============================================================================\n",
      "Omnibus:                        2.940   Durbin-Watson:                   1.985\n",
      "Prob(Omnibus):                  0.230   Jarque-Bera (JB):                2.731\n",
      "Skew:                          -0.158   Prob(JB):                        0.255\n",
      "Kurtosis:                       3.400   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "## Autocorrelation test\n",
    "y = spy_data['Return']\n",
    "x = spy_data['1_Day_Lag_Return']\n",
    "model = sm.OLS(y, x)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our Durbin-Watson statistic is greater than 1.96, we can reject the null hypothesis that there is autocorrellation in our return data.  As above, this does not mean the data is not stationary, but rather, our first test failed to show that it i s. Our next test will be more rigorous.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Root Test\n",
    "The most effective and formal way of testing for stationary data is to use the Dicky-Fuller ters for the existance of unit roots.  Suppose you have the following:\n",
    "![panda-wave](https://wikimedia.org/api/rest_v1/media/math/render/svg/223b7673f7bf871d08b19fb9a3c6ae1f0f619a0b)\n",
    "\n",
    "\n",
    "where ${\\displaystyle \\alpha }$  is a constant, ${\\displaystyle \\beta }$  the coefficient on a time trend and ${\\displaystyle p}$ the lag order of the autoregressive process. Imposing the constraints ${\\displaystyle \\alpha =0}$ and ${\\displaystyle \\beta =0}$ corresponds to modelling a random walk and using the constraint ${\\displaystyle \\beta =0}$ corresponds to modeling a random walk with a drift. Consequently, there are three main versions of the test, analogous to the ones discussed on Dickey–Fuller test (see that page for a discussion on dealing with uncertainty about including the intercept and deterministic time trend terms in the test equation.)\n",
    "\n",
    "By including lags of the order p the ADF formulation allows for higher-order autoregressive processes. This means that the lag length p has to be determined when applying the test. One possible approach is to test down from high orders and examine the t-values on coefficients. An alternative approach is to examine information criteria such as the Akaike information criterion, Bayesian information criterion or the Hannan–Quinn information criterion.\n",
    "\n",
    "The unit root test is then carried out under the null hypothesis ${\\displaystyle \\gamma =0}$ against the alternative hypothesis of ${\\displaystyle \\gamma <0.}$. Once a value for the test statistic\n",
    "\n",
    "${\\displaystyle \\mathrm {DF} _{\\tau }={\\frac {\\hat {\\gamma }}{\\operatorname {SE} ({\\hat {\\gamma }})}}}$\n",
    "is computed it can be compared to the relevant critical value for the Dickey–Fuller test. As this test is asymmetrical, we are only concerned with negative values of our test statistic ${\\displaystyle \\mathrm {DF} _{\\tau }}$. If the calculated test statistic is less (more negative) than the critical value, then the null hypothesis of ${\\displaystyle \\gamma =0}$ is rejected and no unit root is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "## Execute DF test\n",
    "df = sm.tsa.stattools.adfuller(x, maxlag=None, regression='c', autolag='AIC', store=False, regresults=False)[1]\n",
    "print(\"p-value: \"+str(round(df,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our p-value for the unit root test is below .05, we can be reasonably confident that the data we are working with is stationary, and more importantly, that the historicals will be meaningful to estimate the underlying mean and variance of the distribution, though not actually predict what the distribution will do at any specific time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Conclusions\n",
    "At this point, we can be relatively comfortable that our data is stationary, and reasonably resembles a normal distribution.  With this in mind we can proceed to digging into our Portfolio VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR  Single Asset Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our inputs \n",
    "start_value = 10000\n",
    "returns = spy_data[\"Return\"]\n",
    "days_into_future = 252 ##we will assume that we are calculating 1-Year VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a 5% chance that the loss, as measured by VAR, will exceed $1030.63 over the next year for a \n",
      " $10,000 portfolio of SPY.\n"
     ]
    }
   ],
   "source": [
    "vol = returns.T.std()*math.sqrt(days_into_future)  ## annual volatility \n",
    "avg = returns.T.mean()*(days_into_future)  ##annual expected return\n",
    "VAR = start_value *(avg - 1.645*vol)\n",
    "print(\"There is a 5% chance that the loss, as measured by VAR, will exceed $\"+str(-round(VAR,2))+\" over the next year for a \\n $10,000 portfolio of SPY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "While the above can make an investor or analyst more comfortable with an investment opportunity, it is incredibly important to note that this ignores black swan events in the medium term. In the long and short term, markets tend to act normally, but in the medium term, recessions happen.  In any instance of capital planning, be it for an institutional portfolio of commodities, or a personal brokerage account of SPY shares, knowing when liquidity will be needed is just as important as knowing where to invest capital.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Multi-Asset Portfolios\n",
    "While our example focused on a simple single asset portfolio, was it really so simple?  SPY is an aggregation of 505 stocks, and we were able to compute the VAR and run our statistical analysis on a portfolio of all 505 stocks.  As you will be asked to do so on the homework, computing a multi-asset portfio's VAR is just as complicated as what was done above, simply prep the data by finding the portfolio's historical returns to use in the modeling instead of the individual assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
