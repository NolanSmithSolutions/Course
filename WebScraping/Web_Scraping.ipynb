{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![StatModels](https://www.durhamtech.edu/themes/custom/durhamtech/images/durham-tech-logo-web.svg) \n",
    "\n",
    "## Web Scraping\n",
    "\n",
    "This lecture provides foundational knowledge and examples of scraping data from live websites.\n",
    "\n",
    "---\n",
    "\n",
    "### Set Up\n",
    "1.\tEnsure you have a stable internet connection\n",
    "\n",
    "\n",
    "\n",
    "### Needed Packages\n",
    "1.\tpandas\n",
    "2.  selenium\n",
    "3.  bs4\n",
    "4.  requests\n",
    "5.  random\n",
    "6.  time\n",
    "7.  warnings\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### The Basics\n",
    "#### <a href='#1'>What is web scraping and why do you need it?</a>\n",
    "#### <a href='#2'>Needed Packages</a>\n",
    "#### <a href='#3'>Scraping ESPN</a>\n",
    "#### <a href='#4'>A note on anonymous web scraping and browsing</a>\n",
    "\n",
    "### Advanced Methods\n",
    "#### <a href='#5'>Creating an email address through python only</a>\n",
    "#### <a href='#6'>Download Consumer Spending Data</a>\n",
    "#### <a href='#7'>Scrape current US National Debt</a>\n",
    "\n",
    "### Additional Use cases\n",
    "#### <a href='#8'>Scrape images and other files</a>\n",
    "#### <a href='#9'>Scraping function to download files of any type from a website</a>\n",
    "#### <a href='#10'>Scrape funny coffee pictures</a>\n",
    "#### <a href='#11'>Scrape Bloomberg sitemap (XML) for current political news</a>\n",
    "#### <a href='#12'>Web crawl Twitter account</a>\n",
    "\n",
    "### Practice\n",
    "#### <a href='#13'>Exercise Set 1</a>\n",
    "#### <a href='#14'>Exercise Set 2</a>\n",
    "#### <a href='#15'>Exercise Set 3</a>\n",
    "#### <a href='#16'>Exercise Set 4</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## What is web scraping and why do you need it?\n",
    "Web scraping is a catch all term for gathering information directly from web pages on the internet.  Unlike structured APIs and Databases, web scraping will require you to get creative in how you approach collecting the data as no two projects will be the same.  As an important addition to webscraping, this lecture will also cover basic use of the selenium package, which will enable you to tranverse and interact with the vast majority of moder webpages, ranging from obscure pages all the way to Facebook.\n",
    "\n",
    "In general, web scraping should be your last resort when looking to automate a data gathering exercise, as websites are prone to change over time, many web hosts do not appreciate web scraping, and, it's generally much slower than just working with prebuilt data sources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import selenium\n",
    "from bs4 import *\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from random import randint\n",
    "import requests\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## Scraping ESPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define website url\n",
    "url = 'https://www.espn.com/nba/boxscore/_/gameId/401360104'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open browser\n",
    "browser = webdriver.Chrome(executable_path = 'chromedriver.exe')\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Close browser\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open browser without actually seeing it, also known as 'headless' browsing\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "browser.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grab the raw webpage\n",
    "innerHTML = browser.execute_script(\"return document.body.innerHTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parse raw webpage and close the browser (Don't want to gunk up your RAM!!)\n",
    "soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Find all tables on page\n",
    "tables = soup.findAll(\"table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grab Stats for all players on both teams & convert to dataframe\n",
    "df = []\n",
    "for table in tables:\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll(\"td\")\n",
    "        cells = [ele.text.strip() for ele in cells]\n",
    "        if len(cells) == 15 and cells[0] != '' and cells[0] != 'TEAM':\n",
    "            df.append(cells[0:3])\n",
    "df = pd.DataFrame(df, columns = ['Player','Minutes Played','FG'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## A note on anonymous web scraping and browsing\n",
    "When web scraping, many websites will blacklist your IP address in an effort to prevent you from abusing their sites.  If you are at a job and your employer is needing the data, an IP ban is not something you want ruining your day.  A common work around is to simply use a proxy server so that the website doesn't know your actual IP address.  The below will give the framework to put a proxy server between you and the internet calls you are making with python.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Proxy server list\n",
    "## 'https://proxydig.com/free-proxy-list/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Connect to a website using a proxy server\n",
    "PROXY = '209.165.163.187:3128'  ##Note that if you cannot connect to a webpage, try using a different proxy server from the site above\n",
    "url = 'https://whatismyipaddress.com/'\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--proxy-server=%s' % PROXY)\n",
    "browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "##NOTE: Using proxy servers will avoid most common website black listing BUT, does not substitute a VPN for security NOR\n",
    "##should you attempt to use proxy servers for nefarious activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While proxy servers enable you to avoid blacklisting, they leave you exposed from a security standpoint.  There are two common methods to add a layer of security.  The first is to always work with a VPN on before doing any web scraping, this will ensure even if someone tries to trace back to the original IP, only the VPN's will appear.  Alternatively, Selenium allows you to use the Tor browser to make internet calls.  This method is generally extremely slow, and unnecessary, but a fun exercise if you have a couple hours to spare! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='13'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a function that will open and return a headless web browser, think of variables that may be helpful to send the function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Go to: https://www.macrotrends.net/1333/historical-gold-prices-100-year-chart and scrape the table titled 'Gold Prices - Historical Annual Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Put the data from question 2 in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## Creating an email address through python only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open webpage\n",
    "url = 'https://mail.tutanota.com/login'\n",
    "browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select 'More' so we can create an account\n",
    "more_button = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[3]/div/button')\n",
    "more_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select 'Sign Up' so we can create an account\n",
    "sign_up_button = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[4]/div/div/div/button[1]/div')\n",
    "sign_up_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select the Free option\n",
    "free_button = browser.find_element_by_xpath('//*[@id=\"upgrade-account-dialog\"]/div[2]/div[1]/div[1]/div[5]/button/div')\n",
    "free_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Agree to terms\n",
    "term_1 = browser.find_element_by_xpath('//*[@id=\"modal\"]/div[2]/div/div/div/div[2]/div[1]/div/input')\n",
    "term_1.click()\n",
    "term_2 = browser.find_element_by_xpath('//*[@id=\"modal\"]/div[2]/div/div/div/div[2]/div[2]/div/input')\n",
    "term_2.click()\n",
    "ok_button = browser.find_element_by_xpath('//*[@id=\"modal\"]/div[2]/div/div/div/div[3]/button[2]/div')\n",
    "ok_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add Account Info\n",
    "email_add = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[1]/div/div/div/div[1]/input')\n",
    "email_add.click()\n",
    "email_add.send_keys('test_user_abc_123') ###You will need to put in a new username!\n",
    "\n",
    "password = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[2]/div[1]/div/div/div/div[1]/input[4]')\n",
    "password.click()\n",
    "password.send_keys('Sample_password!')\n",
    "\n",
    "sec_password = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[2]/div[3]/div/div/div/div/input')\n",
    "sec_password.click()\n",
    "sec_password.send_keys('Sample_password!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Agree to terms 2\n",
    "term_1 = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[3]/div/input')\n",
    "term_1.click()\n",
    "term_2 = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[4]/div/input')\n",
    "term_2.click()\n",
    "ok_button = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[5]/button/div')\n",
    "ok_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Continue\n",
    "ok_button = browser.find_element_by_xpath('//*[@id=\"wizardDialogContent\"]/div[4]/div/button/div')\n",
    "ok_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Type in Password\n",
    "password_input = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[1]/form/div[2]/div/div/div/div/div/input')\n",
    "password_input.click()\n",
    "password_input.send_keys('Sample_password!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Login\n",
    "login_button = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[1]/form/div[4]/button/div')\n",
    "login_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='14'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Login to your new email account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Click the button to send an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Send yourself an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## Download Consumer Spending Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open Webpage and make full screen\n",
    "url = 'https://www.bea.gov/'\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Data section\n",
    "data = browser.find_element_by_link_text('Data')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Data by Topic section\n",
    "data = browser.find_element_by_partial_link_text('Data by Topic')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Consumer Spending Section\n",
    "data = browser.find_element_by_link_text('Consumer Spending')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to next Consumer Spending Section\n",
    "data = browser.find_element_by_xpath('//*[@id=\"test\"]/div[2]/article/div/div/div/ul/li[1]/a')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Interactive Data\n",
    "data = browser.find_element_by_link_text('Interactive Data')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Summary Tables\n",
    "data = browser.find_element_by_link_text('Summary Tables')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Person Income and Outlays\n",
    "data = browser.find_element_by_partial_link_text('PERSONAL INCOME AND OUTLAYS')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to table 2.2A\n",
    "data = browser.find_element_by_partial_link_text('2.2A')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Download Data\n",
    "data = browser.find_element_by_xpath('//*[@id=\"showDownload\"]')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select Download Format\n",
    "data = browser.find_element_by_xpath('//*[@id=\"download_wraper\"]/div/a[2]')\n",
    "data.click()\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='15'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Navigate to a different data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download a different data set than what was downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read the new data set into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## Scrape current US National Debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open Webpage and make full screen\n",
    "url = 'https://www.usdebtclock.org/'\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grab the raw webpage\n",
    "innerHTML = browser.execute_script(\"return document.body.innerHTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parse raw webpage and close the browser (Don't want to gunk up your RAM!!)\n",
    "soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scrape the First number showing the total debt\n",
    "divs = soup.findAll(\"div\")\n",
    "count = 1\n",
    "for div in divs:\n",
    "    for row in div.findAll(\"span\"):\n",
    "        if count == 1:\n",
    "            print(row.text.strip())\n",
    "            count+=1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## Scrape images and other files\n",
    "Let's see how we can automatically find and download files linked at any website.\n",
    "\n",
    "The data you need for your projects might not always be raw data, but in the form of files (images, .txt files etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see there are two images on the data-x.blog/resources\n",
    "# say that we want to download them\n",
    "# Images are displayed with the <img> tag in HTML\n",
    "\n",
    "# open connection and create new soup\n",
    "\n",
    "raw = requests.get('https://data-x.blog/resources/').content\n",
    "soup = BeautifulSoup(raw,features='html.parser')\n",
    "\n",
    "print(soup.find('img')) \n",
    "# as we can see below the image urls \n",
    "# are stored in the src attribute inside the img tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all url to the images\n",
    "img_urls = list()\n",
    "for img in soup.find_all('img'):\n",
    "    img_url = img.get('src') \n",
    "    if '.jpeg' in img_url or '.jpg' in img_url:\n",
    "        print(img_url)\n",
    "        img_urls.append(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at what our current file directory looks like\n",
    "\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download and save files with Python we can use \n",
    "# the shutil library which is a file operations library\n",
    "'''\n",
    "The shutil module offers a number of high-level operations on files and \n",
    "collections of files. In particular, functions are provided which support \n",
    "file copying and removal.\n",
    "'''\n",
    "\n",
    "import shutil\n",
    "\n",
    "for idx, img_url in enumerate(img_urls): \n",
    "    #enumarte to create a file integer name for every image\n",
    "    \n",
    "    # make a request to the image URL\n",
    "    img_source = requests.get(img_url, stream=True) \n",
    "    # we set stream = True to download/ \n",
    "    # stream the content of the data\n",
    "    \n",
    "    with open('img'+str(idx)+'.jpg', 'wb') as file: \n",
    "        # open file connection, create file and write to it\n",
    "        shutil.copyfileobj(img_source.raw, file) \n",
    "        # save the raw file object\n",
    "\n",
    "    del img_source # to remove the file from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see if the file has been saved\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## Scraping function to download files of any type from a website\n",
    "Below is a function that takes in a website and a specific file type to download X of them from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended scraping function of any file format\n",
    "import os # To interact with operating system and format file name\n",
    "import shutil # To copy file object from python to disk\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "def py_file_scraper(url, html_tag='img', source_tag='src', file_type='.jpg',max=-1):\n",
    "    \n",
    "    '''\n",
    "    Function that scrapes a website for certain file formats.\n",
    "    The files will be placed in a folder called \"files\" \n",
    "    in the working directory.\n",
    "    \n",
    "    url = the url we want to scrape from\n",
    "    html_tag = the file tag (usually img for images or \n",
    "    a for file links)\n",
    "    \n",
    "    source_tag = the source tag for the file url \n",
    "    (usually src for images or href for files)\n",
    "    \n",
    "    file_type = .png, .jpg, .pdf, .csv, .xls etc.\n",
    "    \n",
    "    max = integer (max number of files to scrape, \n",
    "    if = -1 it will scrape all files)\n",
    "    '''\n",
    "    \n",
    "    # make a directory called 'files' \n",
    "    # for the files if it does not exist\n",
    "    if not os.path.exists('files/'):\n",
    "        os.makedirs('files/')\n",
    "    print('Loading content from the url...')\n",
    "    source = requests.get(url).content\n",
    "    print('Creating content soup...')\n",
    "    soup = bs.BeautifulSoup(source,'html.parser')\n",
    "    \n",
    "    i=0\n",
    "    print('Finding tag:%s...'%html_tag)\n",
    "    for n, link in enumerate(soup.find_all(html_tag)):\n",
    "        file_url=link.get(source_tag)\n",
    "        print ('\\n',n+1,'. File url',file_url)\n",
    "        \n",
    "        \n",
    "        if 'http' in file_url: # check that it is a valid link\n",
    "            print('It is a valid url..')\n",
    "            \n",
    "            \n",
    "            if file_type in file_url: #only check for specific \n",
    "                # file type\n",
    "                \n",
    "                print('%s FILE TYPE FOUND IN THE URL...'%file_type)\n",
    "                file_name = os.path.splitext(os.path.basename(file_url))[0] + file_type \n",
    "                #extract file name from url\n",
    "\n",
    "                file_source = requests.get(file_url, stream = True)\n",
    "             \n",
    "                # open new stream connection\n",
    "\n",
    "                with open('./files/'+file_name, 'wb') as file: \n",
    "                    # open file connection, create file and \n",
    "                    # write to it\n",
    "                    \n",
    "                    shutil.copyfileobj(file_source.raw, file) \n",
    "                    # save the raw file object\n",
    "                    \n",
    "                    print('DOWNLOADED:',file_name)\n",
    "                    \n",
    "                    i+=1\n",
    "                    \n",
    "                del file_source # delete from memory\n",
    "            else:\n",
    "                print('%s file type NOT found in url:'%file_type)\n",
    "                print('EXCLUDED:',file_url) \n",
    "                # urls not downloaded from\n",
    "                \n",
    "        if i == max:\n",
    "            print('Max reached')\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## Scrape funny coffee pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_file_scraper('https://goldcoffee.com/dark/') \n",
    "# scrape coffee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## Scrape Bloomberg sitemap (XML) for current political news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML documents - site maps, all the urls. just between tags\n",
    "# XML human and machine readable.\n",
    "# Newest links: all the links for FIND SITE MAP!\n",
    "# News websites will have sitemaps for politics, bot constantly\n",
    "# tracking news track the sitemaps\n",
    "\n",
    "# Before scraping a website look at robots.txt file\n",
    "bs.BeautifulSoup(requests.get('https://www.bloomberg.com/robots.txt').content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get('https://www.bloomberg.com/feeds/bpol/sitemap_news.xml').content\n",
    "soup = bs.BeautifulSoup(source,'xml') # Note parser 'xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find political news headlines\n",
    "for news in soup.find_all({'news'}):\n",
    "    print(news.title.text)\n",
    "    print(news.publication_date.text)\n",
    "    #print(news.keywords.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## Web crawl Twitter account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to maintain the urls and the number of times they appear\n",
    "\n",
    "url_dict = dict()\n",
    "\n",
    "def add_to_dict(url_d, key):\n",
    "    if key in url_d:\n",
    "        url_d[key] = url_d[key] + 1\n",
    "    else:\n",
    "        url_d[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls(url, depth):\n",
    "    if depth == 0:\n",
    "        return\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and \"https://\" in link['href']:\n",
    "#             print(link['href'])\n",
    "            add_to_dict(url_dict, link['href'])\n",
    "            get_urls(link['href'], depth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls_iterative(url, depth):\n",
    "    urls = [url]\n",
    "    for url in urls:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            if link.has_attr('href') and \"https://\" in link['href']:\n",
    "                add_to_dict(url_dict, link['href'])\n",
    "                urls.append(link['href'])\n",
    "        if len(urls) > depth:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls(\"https://twitter.com/GolfWorld\", 2)\n",
    "for key in url_dict:\n",
    "    print(str(key) + \"  ----   \" + str(url_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to a new site and download at least 2 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scrape a different twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the latest bloomberg news topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
