{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "This lecture provides foundational knowledge and examples of scraping/pulling data from live websites.\n",
    "\n",
    "---\n",
    "\n",
    "### Set Up\n",
    "1.\tEnsure you have a stable internet connection\n",
    "2.  Ensure the chromedriver app is in the same folder as this notebook. If you pulled correctly from github, this shouldn't be an issue!\n",
    "\n",
    "\n",
    "### Needed Packages\n",
    "1.\tpandas\n",
    "2.  selenium\n",
    "3.  bs4\n",
    "4.  requests\n",
    "5.  random\n",
    "6.  time\n",
    "7.  warnings\n",
    "\n",
    "### Nice Chrome Extensions to Have\n",
    "1. <a href='https://chrome.google.com/webstore/detail/xpath-finder/ihnknokegkbpmofmafnkoadfjkhlogph?hl=en'>xpath-finder</a>\n",
    "2. <a href='https://chrome.google.com/webstore/detail/relative-xpath-helper/eanaofphbanknlngejejepmfomkjaiic?hl=en'>relative-xpath-helper</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### The Basics\n",
    "#### <a href='#1'>What is web scraping and why do you need it?</a>\n",
    "#### <a href='#2'>Needed Packages</a>\n",
    "#### <a href='#3'>Scraping ESPN</a>\n",
    "#### <a href='#4'>A note on anonymous web scraping and browsing</a>\n",
    "\n",
    "### Advanced Methods\n",
    "#### <a href='#5'>Creating an email address through python only</a>\n",
    "#### <a href='#6'>Download Consumer Spending Data</a>\n",
    "#### <a href='#7'>Scrape current US National Debt</a>\n",
    "\n",
    "### Additional Use cases\n",
    "#### <a href='#8'>Scrape images and other files</a>\n",
    "#### <a href='#9'>Scraping function to download files of any type from a website</a>\n",
    "#### <a href='#10'>Scrape funny coffee pictures</a>\n",
    "#### <a href='#11'>Scrape Bloomberg sitemap (XML) for current political news</a>\n",
    "#### <a href='#12'>Web crawl Twitter account</a>\n",
    "#### <a href='#17'>Making User Entry Hidden (Great for Passwords)</a>\n",
    "#### <a href='#18'>Making Web Browser Wait with Time.Sleep()</a>\n",
    "\n",
    "### Practice\n",
    "#### <a href='#13'>Exercise Set 1</a>\n",
    "#### <a href='#14'>Exercise Set 2</a>\n",
    "#### <a href='#15'>Exercise Set 3</a>\n",
    "#### <a href='#16'>Exercise Set 4</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## What is web scraping and why do you need it?\n",
    "Web scraping is a catch all term for gathering information directly from web pages on the internet.  Unlike structured APIs and Databases, web scraping will require you to get creative in how you approach collecting the data as no two projects will be the same.  As an important addition to webscraping, this lecture will also cover basic use of the selenium package, which will enable you to traverse and interact with the vast majority of modern webpages, ranging from obscure pages all the way to Facebook.\n",
    "\n",
    "In general, web scraping should be your last resort when looking to automate a data gathering exercise, as websites are prone to change over time, many web hosts do not appreciate web scraping, and it's generally much slower than just working with prebuilt data sources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import selenium\n",
    "from bs4 import *\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import getpass\n",
    "from random import randint\n",
    "import requests\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a Mac, need to specify path that contains chromedriver\n",
    "driver_path = \"/Users/Matthew/Documents/Consulting/Lectures/Webscraping/chromedriver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## Scraping ESPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define website url\n",
    "url = 'https://www.espn.com/nba/boxscore/_/gameId/401360104'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open browser\n",
    "\n",
    "# If using Mac\n",
    "#browser = webdriver.Chrome(executable_path = 'chromedriver')\n",
    "\n",
    "# If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path)\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Close browser\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open browser without actually seeing it, also known as 'headless' browsing\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "#If using Mac\n",
    "#browser = webdriver.Chrome(executable_path = 'chromedriver.exe', chrome_options=chrome_options)\n",
    "\n",
    "#If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grab the raw webpage\n",
    "innerHTML = browser.execute_script(\"return document.body.innerHTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parse raw webpage and close the browser (Don't want to gunk up your RAM!!)\n",
    "soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Find all tables on page\n",
    "tables = soup.findAll(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Minutes Played</th>\n",
       "      <th>FG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>J. GrantJ. GrantSF</td>\n",
       "      <td>33</td>\n",
       "      <td>5-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>S. BeyS. BeySF</td>\n",
       "      <td>23</td>\n",
       "      <td>4-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I. StewartI. StewartC</td>\n",
       "      <td>27</td>\n",
       "      <td>2-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>C. JosephC. JosephPG</td>\n",
       "      <td>25</td>\n",
       "      <td>3-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>C. CunninghamC. CunninghamPG</td>\n",
       "      <td>28</td>\n",
       "      <td>3-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Player Minutes Played    FG\n",
       "0            J. GrantJ. GrantSF             33  5-16\n",
       "1                S. BeyS. BeySF             23  4-12\n",
       "2         I. StewartI. StewartC             27   2-6\n",
       "3          C. JosephC. JosephPG             25   3-6\n",
       "4  C. CunninghamC. CunninghamPG             28  3-13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Grab Stats for all players on both teams & convert to dataframe\n",
    "df = []\n",
    "for table in tables:\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll(\"td\")\n",
    "        cells = [ele.text.strip() for ele in cells]\n",
    "        if len(cells) == 15 and cells[0] != '' and cells[0] != 'TEAM':\n",
    "            df.append(cells[0:3])\n",
    "df = pd.DataFrame(df, columns = ['Player','Minutes Played','FG'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## A note on anonymous web scraping and browsing\n",
    "When web scraping, many websites will blacklist your IP address in an effort to prevent you from abusing their sites.  If you are at a job and your employer is needing the data, an IP ban is not something you want ruining your day.  A common work around is to simply use a proxy server so that the website doesn't know your actual IP address.  The below will give the framework to put a proxy server between you and the internet calls you are making with python.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Proxy server list\n",
    "## 'https://proxydig.com/free-proxy-list/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_PROXY_CONNECTION_FAILED\n  (Session info: chrome=101.0.4951.54)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5ad0d80847d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutable_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchrome_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m##NOTE: Using proxy servers will avoid most common website black listing BUT, does not substitute a VPN for security NOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_PROXY_CONNECTION_FAILED\n  (Session info: chrome=101.0.4951.54)\n"
     ]
    }
   ],
   "source": [
    "##Connect to a website using a proxy server\n",
    "PROXY = '209.165.163.187:3128'  ##Note that if you cannot connect to a webpage, try using a different proxy server from the site above\n",
    "url = 'https://whatismyipaddress.com/'\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--proxy-server=%s' % PROXY)\n",
    "\n",
    "# If using Mac\n",
    "#browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "\n",
    "#If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "\n",
    "browser.get(url)\n",
    "\n",
    "##NOTE: Using proxy servers will avoid most common website black listing BUT, does not substitute a VPN for security NOR\n",
    "##should you attempt to use proxy servers for nefarious activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While proxy servers enable you to avoid blacklisting, they leave you exposed from a security standpoint.  There are two common methods to add a layer of security.  The first is to always work with a VPN on before doing any web scraping, this will ensure even if someone tries to trace back to the original IP, only the VPN's will appear.  Alternatively, Selenium allows you to use the Tor browser to make internet calls.  This method is generally extremely slow, and unnecessary, but a fun exercise if you have a couple hours to spare! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='13'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a function that will open and return a headless web browser, think of variables that may be helpful to send the function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Go to: https://www.macrotrends.net/1333/historical-gold-prices-100-year-chart and scrape the table titled 'Gold Prices - Historical Annual Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Put the data from question 2 in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## Creating an email address through python only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open webpage\n",
    "url = 'https://mail.tutanota.com/login'\n",
    "\n",
    "# If using Mac\n",
    "#browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "\n",
    "# If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select 'More' so we can create an account\n",
    "more_button = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[3]/div/button')\n",
    "more_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select 'Sign Up' so we can create an account\n",
    "sign_up_button = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[4]/div/div/div/button[1]/div')\n",
    "sign_up_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select the Free option\n",
    "free_button = browser.find_element_by_xpath('//*[@id=\"upgrade-account-dialog\"]/div[2]/div[1]/div[1]/div[5]/button/div')\n",
    "free_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Agree to terms\n",
    "term_1 = browser.find_element_by_xpath('//*[@id=\"modal\"]/div[2]/div/div/div/div[2]/div[1]/div/input')\n",
    "term_1.click()\n",
    "term_2 = browser.find_element_by_xpath('//*[@id=\"modal\"]/div[2]/div/div/div/div[2]/div[2]/div/input')\n",
    "term_2.click()\n",
    "ok_button = browser.find_element_by_xpath('//*[@id=\"modal\"]/div[2]/div/div/div/div[3]/button[2]/div')\n",
    "ok_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add Account Info\n",
    "email_add = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[1]/div/div/div/div[1]/input')\n",
    "email_add.click()\n",
    "email_add.send_keys('test_user_abc_124') ###You will need to put in a new username!\n",
    "\n",
    "password = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[2]/div[1]/div/div/div/div[1]/input[4]')\n",
    "password.click()\n",
    "password.send_keys('Sample_password!')\n",
    "\n",
    "sec_password = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[2]/div[3]/div/div/div/div/input')\n",
    "sec_password.click()\n",
    "sec_password.send_keys('Sample_password!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Agree to terms 2\n",
    "term_1 = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[3]/div/input')\n",
    "term_1.click()\n",
    "term_2 = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[4]/div/input')\n",
    "term_2.click()\n",
    "ok_button = browser.find_element_by_xpath('//*[@id=\"signup-account-dialog\"]/div/div[5]/button/div')\n",
    "ok_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Continue\n",
    "ok_button = browser.find_element_by_xpath('//*[@id=\"wizardDialogContent\"]/div[4]/div/button/div')\n",
    "ok_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Type in Password\n",
    "password_input = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[1]/form/div[2]/div/div/div/div/div/input')\n",
    "password_input.click()\n",
    "password_input.send_keys('Sample_password!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Login\n",
    "login_button = browser.find_element_by_xpath('//*[@id=\"login-view\"]/div[2]/div/div[1]/form/div[4]/button/div')\n",
    "login_button.click()\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='14'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Login to your new email account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Click the button to send an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Send yourself an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## Download Consumer Spending Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open Webpage and make full screen\n",
    "url = 'https://www.bea.gov/'\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "\n",
    "# If using Mac\n",
    "#browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "\n",
    "# If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Data section\n",
    "data = browser.find_element_by_link_text('Data')\n",
    "data.click()\n",
    "\n",
    "##Note that if you don't leave the browser in max view, this won't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Data by Topic section\n",
    "data = browser.find_element_by_partial_link_text('Data by Topic')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Consumer Spending Section\n",
    "data = browser.find_element_by_link_text('Consumer Spending')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to next Consumer Spending Section\n",
    "data = browser.find_element_by_xpath('//*[@id=\"test\"]/div[2]/article/div/div/div/ul/li[1]/a')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Interactive Data\n",
    "data = browser.find_element_by_link_text('Interactive Data')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Summary Tables\n",
    "data = browser.find_element_by_link_text('Summary Tables')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to Person Income and Outlays\n",
    "data = browser.find_element_by_partial_link_text('PERSONAL INCOME AND OUTLAYS')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Navigate to table 2.2A\n",
    "data = browser.find_element_by_partial_link_text('2.2A')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Download Data\n",
    "data = browser.find_element_by_xpath('//*[@id=\"showDownload\"]')\n",
    "data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select Download Format\n",
    "data = browser.find_element_by_xpath('//*[@id=\"download_wraper\"]/div/a[2]')\n",
    "data.click()\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='15'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Navigate to a different data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download a different data set than what was downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read the new data set into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## Scrape current US National Debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Open Webpage and make full screen\n",
    "url = 'https://www.usdebtclock.org/'\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "# If using Mac\n",
    "#browser = webdriver.Chrome(executable_path = 'chromedriver.exe',chrome_options=chrome_options)\n",
    "\n",
    "# If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grab the raw webpage\n",
    "innerHTML = browser.execute_script(\"return document.body.innerHTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parse raw webpage and close the browser (Don't want to gunk up your RAM!!)\n",
    "soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scrape the First number showing the total debt\n",
    "divs = soup.findAll(\"div\")\n",
    "count = 1\n",
    "for div in divs:\n",
    "    for row in div.findAll(\"span\"):\n",
    "        if count == 1:\n",
    "            print(row.text.strip())\n",
    "            count+=1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## Scrape images and other files\n",
    "Let's see how we can automatically find and download files linked at any website.\n",
    "\n",
    "The data you need for your projects might not always be raw data, but in the form of files (images, .txt files etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images are displayed with the <img> tag in HTML\n",
    "\n",
    "# open connection and create new soup\n",
    "\n",
    "raw = requests.get('https://www.eatthis.com/unhealthiest-foods-on-the-planet/').content\n",
    "soup = BeautifulSoup(raw,features='html.parser')\n",
    "\n",
    "print(soup.find('img')) \n",
    "# as we can see below the image urls \n",
    "# are stored in the src attribute inside the img tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all url to the images\n",
    "img_urls = list()\n",
    "for img in soup.find_all('img'):\n",
    "    img_url = img.get('src') \n",
    "    if '.jpeg' in img_url or '.jpg' in img_url:\n",
    "        print(img_url)\n",
    "        img_urls.append(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at what our current file directory looks like\n",
    "\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download and save files with Python we can use \n",
    "# the shutil library which is a file operations library\n",
    "'''\n",
    "The shutil module offers a number of high-level operations on files and \n",
    "collections of files. In particular, functions are provided which support \n",
    "file copying and removal.\n",
    "'''\n",
    "\n",
    "import shutil\n",
    "\n",
    "for idx, img_url in enumerate(img_urls): \n",
    "    #enumarte to create a file integer name for every image\n",
    "    \n",
    "    # make a request to the image URL\n",
    "    img_source = requests.get(img_url, stream=True) \n",
    "    # we set stream = True to download/ \n",
    "    # stream the content of the data\n",
    "    \n",
    "    with open('Spam/img'+str(idx)+'.jpg', 'wb') as file: \n",
    "        # open file connection, create file and write to it\n",
    "        shutil.copyfileobj(img_source.raw, file) \n",
    "        # save the raw file object\n",
    "\n",
    "    del img_source # to remove the file from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see if the file has been saved\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## Scraping function to download files of any type from a website\n",
    "Below is a function that takes in a website and a specific file type to download X of them from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended scraping function of any file format\n",
    "import os # To interact with operating system and format file name\n",
    "import shutil # To copy file object from python to disk\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "# source: https://github.com/scetx/datax/blob/master/03-data-x-data-handling/m310-tools-webscraping/nb-m310-tools-webscraping.ipynb\n",
    "def py_file_scraper(url, html_tag='img', source_tag='src', file_type='.jpg',max=-1):\n",
    "    \n",
    "    '''\n",
    "    Function that scrapes a website for certain file formats.\n",
    "    The files will be placed in a folder called \"files\" \n",
    "    in the working directory.\n",
    "    \n",
    "    url = the url we want to scrape from\n",
    "    html_tag = the file tag (usually img for images or \n",
    "    a for file links)\n",
    "    \n",
    "    source_tag = the source tag for the file url \n",
    "    (usually src for images or href for files)\n",
    "    \n",
    "    file_type = .png, .jpg, .pdf, .csv, .xls etc.\n",
    "    \n",
    "    max = integer (max number of files to scrape, \n",
    "    if = -1 it will scrape all files)\n",
    "    '''\n",
    "    \n",
    "    # make a directory called 'files' \n",
    "    # for the files if it does not exist\n",
    "    if not os.path.exists('files/'):\n",
    "        os.makedirs('files/')\n",
    "    print('Loading content from the url...')\n",
    "    source = requests.get(url).content\n",
    "    print('Creating content soup...')\n",
    "    soup = bs.BeautifulSoup(source,'html.parser')\n",
    "    \n",
    "    i=0\n",
    "    print('Finding tag:%s...'%html_tag)\n",
    "    for n, link in enumerate(soup.find_all(html_tag)):\n",
    "        file_url=link.get(source_tag)\n",
    "        print ('\\n',n+1,'. File url',file_url)\n",
    "        \n",
    "        \n",
    "        if 'http' in file_url: # check that it is a valid link\n",
    "            print('It is a valid url..')\n",
    "            \n",
    "            \n",
    "            if file_type in file_url: #only check for specific \n",
    "                # file type\n",
    "                \n",
    "                print('%s FILE TYPE FOUND IN THE URL...'%file_type)\n",
    "                file_name = os.path.splitext(os.path.basename(file_url))[0] + file_type \n",
    "                #extract file name from url\n",
    "\n",
    "                file_source = requests.get(file_url, stream = True)\n",
    "             \n",
    "                # open new stream connection\n",
    "\n",
    "                with open('./files/'+file_name, 'wb') as file: \n",
    "                    # open file connection, create file and \n",
    "                    # write to it\n",
    "                    \n",
    "                    shutil.copyfileobj(file_source.raw, file) \n",
    "                    # save the raw file object\n",
    "                    \n",
    "                    print('DOWNLOADED:',file_name)\n",
    "                    \n",
    "                    i+=1\n",
    "                    \n",
    "                del file_source # delete from memory\n",
    "            else:\n",
    "                print('%s file type NOT found in url:'%file_type)\n",
    "                print('EXCLUDED:',file_url) \n",
    "                # urls not downloaded from\n",
    "                \n",
    "        if i == max:\n",
    "            print('Max reached')\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## Scrape funny coffee pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_file_scraper('https://goldcoffee.com/dark/') \n",
    "# scrape coffee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## Scrape Bloomberg sitemap (XML) for current political news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML documents - site maps, all the urls. just between tags\n",
    "# XML human and machine readable.\n",
    "# Newest links: all the links for FIND SITE MAP!\n",
    "# News websites will have sitemaps for politics, bot constantly\n",
    "# tracking news track the sitemaps\n",
    "\n",
    "# Before scraping a website look at robots.txt file\n",
    "bs.BeautifulSoup(requests.get('https://www.bloomberg.com/robots.txt').content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get('https://www.bloomberg.com/feeds/bpol/sitemap_news.xml').content\n",
    "soup = bs.BeautifulSoup(source,'xml') # Note parser 'xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find political news headlines\n",
    "for news in soup.find_all({'news'}):\n",
    "    print(news.title.text)\n",
    "    print(news.publication_date.text)\n",
    "    #print(news.keywords.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## Web crawl Twitter account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to maintain the urls and the number of times they appear\n",
    "\n",
    "url_dict = dict()\n",
    "\n",
    "def add_to_dict(url_d, key):\n",
    "    if key in url_d:\n",
    "        url_d[key] = url_d[key] + 1\n",
    "    else:\n",
    "        url_d[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls(url, depth):\n",
    "    if depth == 0:\n",
    "        return\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and \"https://\" in link['href']:\n",
    "#             print(link['href'])\n",
    "            add_to_dict(url_dict, link['href'])\n",
    "            get_urls(link['href'], depth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls_iterative(url, depth):\n",
    "    urls = [url]\n",
    "    for url in urls:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            if link.has_attr('href') and \"https://\" in link['href']:\n",
    "                add_to_dict(url_dict, link['href'])\n",
    "                urls.append(link['href'])\n",
    "        if len(urls) > depth:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls(\"https://twitter.com/GolfWorld\", 2)\n",
    "for key in url_dict:\n",
    "    print(str(key) + \"  ----   \" + str(url_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17'></a>\n",
    "## Making User Entry Hidden (Great for Passwords)\n",
    "\n",
    "getpass() prompts the user for a password without echoing. The getpass module provides a secure way to handle the password prompts where programs interact with the users via the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import this package to have the hidden password functionality\n",
    "import getpass\n",
    " \n",
    "# Set password as variable p, put your question as a string for the function argument\n",
    "# Notice that the password is not shown when entered\n",
    "p = getpass.getpass(\"What is your password? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The password is stored within the memory of the program though\n",
    "print('Password entered:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18'></a>\n",
    "## Making Web Browser Wait with Time.Sleep()\n",
    "\n",
    "Python time method sleep() suspends execution for the given number of seconds. The argument may be a floating point number to indicate a more precise sleep time.  The actual suspension time may be less than that requested because any caught signal will terminate the sleep() following execution of that signal's catching routine.\n",
    "\n",
    "source: https://www.tutorialspoint.com/python/time_sleep.htm\n",
    "\n",
    "This function is very important to use in automation in conjunction with chromedriver.  Some elements on a web page may take a few seconds for your computer to load and trying to access them before that can result in an error.  This function tells Python to wait a certain number of seconds before it tries to run the next command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time in seconds that we want to wait as 8 and store in variable t\n",
    "t = 8\n",
    "\n",
    "print('We have set the the variable t')\n",
    "\n",
    "# Simply call time.sleep() with the seconds to wait as the function's argument\n",
    "# Here we are waiting 8 seconds\n",
    "time.sleep(t)\n",
    "\n",
    "print('It has been 8 seconds')\n",
    "\n",
    "# Or simply just use an integer as the argument\n",
    "# Here we are waiting 5 seconds\n",
    "time.sleep(5)\n",
    "print('It has been 5 more seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16'></a>\n",
    "## -------------PRACTICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to a new site and download at least 2 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scrape a different twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the latest bloomberg news topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings:\n",
    "\n",
    "Thinks to keep in mind as you scrape data: https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
