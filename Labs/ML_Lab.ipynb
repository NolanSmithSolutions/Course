{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the 2 cells below before you start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "bc = load_breast_cancer()\n",
    "target_names = list(bc.target_names)\n",
    "print(target_names)\n",
    "\n",
    "# Put data into datarame\n",
    "df = pd.DataFrame(bc.data,columns=bc.feature_names)\n",
    "\n",
    "# Add dependent variable\n",
    "df['Benign'] = bc.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset description - unique to sklearn datasets\n",
    "print(bc.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the cell below, display the number of non-null entries for each column and the datatype for each column.  (hint: is there an easy function that does this in a simple command?)  Are there any null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the cell below, display the count, mean, min, and max for each column. (hint: is there an easy function that does this in a simple command?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the data into a training and testing set with 25% of data being used as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df.pop('Benign')\n",
    "X=df.copy()\n",
    "x_train, x_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the cell below.  Are there any models that overfit the training data?  Why can we use the Perceptron and Logistic Regression models (hint: what possible values does our dependent variable take?)  How many clusters are we grouping the data into with our KNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print(\"KNN results:\")\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)                  \n",
    "knn.fit(x_train, y_train)                                    \n",
    "knn_train_acc = knn.score(x_train, y_train)\n",
    "knn_test_acc = knn.score(x_test, y_test)\n",
    "print ('kNN training acuracy= ',knn_train_acc)\n",
    "print('kNN test accuracy= ',knn_test_acc)\n",
    "\n",
    "print(\"\\nSVM results:\")\n",
    "svc = SVC()                                                  \n",
    "svc.fit(x_train, y_train)                                    \n",
    "svc_train_acc = svc.score(x_train, y_train)\n",
    "svc_test_acc = svc.score(x_test, y_test)\n",
    "print ('SVM training acuracy= ',svc_train_acc)\n",
    "print('SVM test accuracy= ',svc_test_acc)\n",
    "\n",
    "print(\"\\nLogistic results:\")\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "logreg_train_acc = logreg.score(x_train, y_train)\n",
    "logreg_test_acc = logreg.score(x_test, y_test)\n",
    "print ('Logistic Regression training acuracy= ',logreg_train_acc)\n",
    "print('Logistic Regression test accuracy= ',logreg_test_acc)\n",
    "\n",
    "print(\"\\nPerceptron results:\")\n",
    "perceptron = Perceptron(max_iter=20)\n",
    "perceptron.fit(x_train, y_train)\n",
    "perceptron_train_acc = perceptron.score(x_train, y_train)\n",
    "perceptron_test_acc = perceptron.score(x_test, y_test)\n",
    "print ('perceptron training acuracy= ',perceptron_train_acc)\n",
    "print('perceptron test accuracy= ',perceptron_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run the cell below.  Do any models overfit their training data?  Why are we using a classifier model as opposed to a regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"CART results:\")\n",
    "cart = DecisionTreeClassifier(random_state=12,max_depth=4, min_samples_split=10)\n",
    "cart.fit(x_train, y_train)\n",
    "train_acc = cart.score(x_train, y_train)\n",
    "test_acc = cart.score(x_test, y_test)\n",
    "y_pred_cart = cart.predict(x_test)\n",
    "print('CART training acuracy= ',train_acc)\n",
    "print('CART test accuracy= ',test_acc)\n",
    "\n",
    "print(\"\\nRandom Forest results:\")\n",
    "random_forest = RandomForestClassifier(n_estimators=12)\n",
    "random_forest.fit(x_train, y_train)\n",
    "train_acc = random_forest.score(x_train, y_train)\n",
    "test_acc = random_forest.score(x_test, y_test)\n",
    "y_pred_rf = random_forest.predict(x_test)\n",
    "print('Training acuracy= ',train_acc)\n",
    "print('Test accuracy= ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Print out the feature importances of the random forest model.  Do the most important features make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Run the cell below.  Do the splits make sense when comparing to the feature importances?  What does the top line of each box indicate?  What about samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image, display\n",
    "import pydotplus\n",
    "\n",
    "# Creates tree graph output, used in the CART function\n",
    "def jupyter_graphviz(m, **kwargs):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(m, dot_data, **kwargs)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    display(Image(graph.create_png()))\n",
    "\n",
    "jupyter_graphviz(cart, filled=True, rounded=True, special_characters=True, feature_names=x_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. In the cell below, create a confusion matrix on our CART model.  Does anything standout to you when analyzing it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## Confusion Matrix!\n",
    "ConfusionMatrix=pd.DataFrame(confusion_matrix(..., ...),\n",
    "                             columns=['Predicted Malignant','Predicted Benign'],\n",
    "                             index=['Actual Malignant','Actual Benign'])\n",
    "print ('Confusion matrix of test data is: \\n', ConfusionMatrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
